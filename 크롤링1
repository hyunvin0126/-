# ê°œë°œí™œê²½ ì„¤ì •
pip install requests beautifulsoup4 pandas openpyxl

"""
í•œêµ­ ëŒ€ê¸°ì—… ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬
ì¡ì½”ë¦¬ì•„, ì‚¬ëŒì¸ì—ì„œ ëŒ€ê¸°ì—… ì±„ìš© ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime
import json

class JobCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.jobs = []

    def crawl_jobkorea(self, keyword="ëŒ€ê¸°ì—…", pages=3):
        """ì¡ì½”ë¦¬ì•„ì—ì„œ ì±„ìš©ê³µê³  í¬ë¡¤ë§"""
        print(f"ğŸ” ì¡ì½”ë¦¬ì•„ì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")

        for page in range(1, pages + 1):
            try:
                # ëŒ€ê¸°ì—… í•„í„°: cotype=1,2,3 (ëŒ€ê¸°ì—…, ì¤‘ê²¬ê¸°ì—…, ê°•ì†Œê¸°ì—…)
                url = f"https://www.jobkorea.co.kr/Search/?stext={keyword}&tabType=recruit&Page_No={page}&cotype=1,2,3"

                response = requests.get(url, headers=self.headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')

                # ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                job_list = soup.select('div.post-list-info')

                for job in job_list:
                    try:
                        # íšŒì‚¬ëª…
                        company = job.select_one('a.name')
                        company_name = company.text.strip() if company else "ì •ë³´ ì—†ìŒ"

                        # ê³µê³  ì œëª©
                        title = job.select_one('a.title')
                        job_title = title.text.strip() if title else "ì •ë³´ ì—†ìŒ"
                        job_url = "https://www.jobkorea.co.kr" + title['href'] if title else ""

                        # ê²½ë ¥/í•™ë ¥
                        conditions = job.select('p.option span')
                        experience = conditions[0].text.strip() if len(conditions) > 0 else "ì •ë³´ ì—†ìŒ"
                        education = conditions[1].text.strip() if len(conditions) > 1 else "ì •ë³´ ì—†ìŒ"
                        location = conditions[2].text.strip() if len(conditions) > 2 else "ì •ë³´ ì—†ìŒ"

                        # ë§ˆê°ì¼
                        deadline = job.select_one('span.date')
                        deadline_text = deadline.text.strip() if deadline else "ì •ë³´ ì—†ìŒ"

                        self.jobs.append({
                            'ì¶œì²˜': 'ì¡ì½”ë¦¬ì•„',
                            'íšŒì‚¬ëª…': company_name,
                            'ê³µê³ ì œëª©': job_title,
                            'ê²½ë ¥': experience,
                            'í•™ë ¥': education,
                            'ì§€ì—­': location,
                            'ë§ˆê°ì¼': deadline_text,
                            'URL': job_url,
                            'ìˆ˜ì§‘ì¼ì‹œ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                    except Exception as e:
                        print(f"âš ï¸ ê°œë³„ ê³µê³  ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue

                print(f"âœ… ì¡ì½”ë¦¬ì•„ {page}í˜ì´ì§€ ì™„ë£Œ ({len(self.jobs)}ê°œ)")
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

            except Exception as e:
                print(f"âŒ ì¡ì½”ë¦¬ì•„ {page}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                continue

    def crawl_saramin(self, keyword="ëŒ€ê¸°ì—…", pages=3):
        """ì‚¬ëŒì¸ì—ì„œ ì±„ìš©ê³µê³  í¬ë¡¤ë§"""
        print(f"ğŸ” ì‚¬ëŒì¸ì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")

        for page in range(1, pages + 1):
            try:
                # ëŒ€ê¸°ì—… í•„í„°: company_cd=0,1,2,3 (ëŒ€ê¸°ì—… ê³„ì—´ì‚¬, ëŒ€ê¸°ì—…, ì¤‘ê²¬ê¸°ì—… ë“±)
                url = f"https://www.saramin.co.kr/zf_user/search/recruit?searchType=search&searchword={keyword}&recruitPage={page}&recruitSort=reg_dt&recruitPageCount=40&company_cd=0,1,2,3"

                response = requests.get(url, headers=self.headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')

                # ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
                job_list = soup.select('div.item_recruit')

                for job in job_list:
                    try:
                        # íšŒì‚¬ëª…
                        company = job.select_one('strong.corp_name a')
                        company_name = company.text.strip() if company else "ì •ë³´ ì—†ìŒ"

                        # ê³µê³  ì œëª©
                        title = job.select_one('h2.job_tit a')
                        job_title = title.text.strip() if title else "ì •ë³´ ì—†ìŒ"
                        job_url = "https://www.saramin.co.kr" + title['href'] if title else ""

                        # ì¡°ê±´ë“¤
                        conditions = job.select('div.job_condition span')
                        condition_text = [c.text.strip() for c in conditions]

                        experience = condition_text[0] if len(condition_text) > 0 else "ì •ë³´ ì—†ìŒ"
                        education = condition_text[1] if len(condition_text) > 1 else "ì •ë³´ ì—†ìŒ"
                        employment_type = condition_text[2] if len(condition_text) > 2 else "ì •ë³´ ì—†ìŒ"

                        # ì§€ì—­
                        location = job.select_one('div.job_condition p.work_place')
                        location_text = location.text.strip() if location else "ì •ë³´ ì—†ìŒ"

                        # ë§ˆê°ì¼
                        deadline = job.select_one('span.date')
                        deadline_text = deadline.text.strip() if deadline else "ì •ë³´ ì—†ìŒ"

                        self.jobs.append({
                            'ì¶œì²˜': 'ì‚¬ëŒì¸',
                            'íšŒì‚¬ëª…': company_name,
                            'ê³µê³ ì œëª©': job_title,
                            'ê²½ë ¥': experience,
                            'í•™ë ¥': education,
                            'ê³ ìš©í˜•íƒœ': employment_type,
                            'ì§€ì—­': location_text,
                            'ë§ˆê°ì¼': deadline_text,
                            'URL': job_url,
                            'ìˆ˜ì§‘ì¼ì‹œ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
                    except Exception as e:
                        print(f"âš ï¸ ê°œë³„ ê³µê³  ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue

                print(f"âœ… ì‚¬ëŒì¸ {page}í˜ì´ì§€ ì™„ë£Œ ({len(self.jobs)}ê°œ)")
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

            except Exception as e:
                print(f"âŒ ì‚¬ëŒì¸ {page}í˜ì´ì§€ ì˜¤ë¥˜: {e}")
                continue

    def save_to_csv(self, filename=None):
        """ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥"""
        if not self.jobs:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            filename = f"ì±„ìš©ê³µê³ _{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        df = pd.DataFrame(self.jobs)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ {len(self.jobs)}ê°œ ê³µê³ ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def save_to_excel(self, filename=None):
        """ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ Excelë¡œ ì €ì¥"""
        if not self.jobs:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            filename = f"ì±„ìš©ê³µê³ _{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

        df = pd.DataFrame(self.jobs)
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"ğŸ’¾ {len(self.jobs)}ê°œ ê³µê³ ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def display_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½"""
        if not self.jobs:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.jobs)
        print("\n" + "="*60)
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        print(f"ì´ ê³µê³  ìˆ˜: {len(self.jobs)}ê°œ")
        print(f"\nì¶œì²˜ë³„ ë¶„í¬:")
        print(df['ì¶œì²˜'].value_counts())
        print(f"\níšŒì‚¬ë³„ ê³µê³  ìˆ˜ (Top 10):")
        print(df['íšŒì‚¬ëª…'].value_counts().head(10))
        print("="*60 + "\n")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = JobCrawler()

    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì„¤ì •
    keyword = "ë¬´ì—­"  # ì›í•˜ëŠ” í‚¤ì›Œë“œë¡œ ë³€ê²½ ê°€ëŠ¥

    # ì¡ì½”ë¦¬ì•„ í¬ë¡¤ë§ (3í˜ì´ì§€)
    crawler.crawl_jobkorea(keyword=keyword, pages=3)

    # ì‚¬ëŒì¸ í¬ë¡¤ë§ (3í˜ì´ì§€)
    crawler.crawl_saramin(keyword=keyword, pages=3)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    crawler.display_summary()

    # CSV íŒŒì¼ë¡œ ì €ì¥
    crawler.save_to_csv()

    # Excel íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)
    # crawler.save_to_excel()

    print("âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
